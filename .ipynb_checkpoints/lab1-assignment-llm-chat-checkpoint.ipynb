{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a403c511-01b1-41d3-856e-3d5f86b30875",
   "metadata": {},
   "source": [
    "# Lab1 Assignment: have a conversation with a Large Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8369cd6b-573d-43de-a4b8-170ad6c30a28",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb92fb8b-6e76-4a6b-86b3-76db5a46f0da",
   "metadata": {},
   "source": [
    "Through this notebook, you will chat with a Generative Large Language Model (LLM). \n",
    "We will install a local server **ollama** through which we can download and use a large variety of open source models. Although you can chat directly with **ollama**, we will use a simple Python script that acts as a chat client so that we can save the conversation as data annd use it later in the course.\n",
    "\n",
    "For the first assignment (see Canvas), you need to have a conversation with a generative LLM that lasts at least 40 turns (20 from the LLM and 20 from you). When you have a conversation do NOT give any personal details but act as a fake person with a fake name making up a story. Try to be emotional and show diverse emotions in your input: make it an emotional roller coaster. You stop the conversation by typing one of the following words: [\"quit\", \"exit\", \"bye\", \"stop\"]. After stopping the conversation will be saved in a file that you need for your assignment.\n",
    "\n",
    "We will now first guide you through installing the **ollama** server and downloading models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a9529-7d86-4a6b-9397-080da56bb066",
   "metadata": {},
   "source": [
    "## Setting up a local server for LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24f762c-22b6-4200-a7d8-975b62f581e9",
   "metadata": {},
   "source": [
    "There are many open source models available and the smaller ones you can load in the memory of a local computer. \n",
    "\n",
    "There are also various ways to run these models locally. We will use the [ollama](https://ollama.com) package to download and use Generative LLMs. For this you need to go through the following steps:\n",
    "\n",
    "1. Download and run the **ollama** server installer from their [website](https://ollama.com/download). There are installers for Mac, Linux and Windows.\n",
    "2. After installing the server you can pull any [model](https://ollama.com/search) that they support.\n",
    "\n",
    "The next command pulls the smallest **Qwen3** model (523MB) from the website and makes it available to your local server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "260b15e4-445c-4b9b-a12f-f9bec6160800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ollama' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!ollama pull qwen3:0.6b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d236b8f6-cdfe-42e0-bdcb-010f723cf089",
   "metadata": {},
   "source": [
    "You can repeat this for every model that you want to install locally. Obviously, you need to have sufficient disk space to store it and sufficient RAM memory to load it. The bigger the model, the better the performance but for this course it is fine to work with a small model.\n",
    "\n",
    "The client uses the **qwen3:1.7b** model as the default. This model 1.4GB in size and may probably also work on your machine. Use the next command to find out which models you have locally available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "095c0798-6ed5-41a2-8b00-f07855e4682c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ollama' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b22837a-2841-43fa-8983-d501e527573d",
   "metadata": {},
   "source": [
    "If you run out of disk space you can easily remove a model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "256d54af-d1b5-4565-96c5-791f9de1d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ollama rm llama3.2:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b5d5f1-ba69-4619-bc76-bbcb867a8647",
   "metadata": {},
   "source": [
    "## Problem shooting\n",
    "\n",
    "It may be the case that your environment cannot find the \"ollama\" executable, especially on Windows when the environment variable was not adapted correctly during the installation.\n",
    "\n",
    "You can execute the ```ollama``` commands also from a terminal outside the notebook. If the command is not recognized from the terminal try to call it by specifying the full path to the executable on Windows, e.g.:\n",
    "\n",
    "    ```C:\\Users\\MY_USER\\AppData\\Local\\Programs\\Ollama\\ollama pull qwen3:0.6b```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db241d67-e9eb-4fa1-9d47-b756fccb4453",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (3426564375.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m'''C:\\Users\\manar\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Ollama\\ollama pull qwen3:0.6b'''\u001b[39m\n                                                                                                            ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "C:\\Users\\manar\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Ollama\\ollama pull qwen3:0.6b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd230988-2c7b-4018-9131-ca6d4f912db7",
   "metadata": {},
   "source": [
    "## Using the LLM chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269c3b9b-cb62-468f-8c95-cb036d53562b",
   "metadata": {},
   "source": [
    "All the code and functions for the chatbot client are given in the **llm_client.py** file. This file needs to be located in the same directory as this notebook. We will load the scripts from this file to create an instance of the chatbot and call its functions.\n",
    "\n",
    "You should already have installed the **ollama**, **langchain**, and **langchain_ollama** Python packages, which are used by the client. If not install these through the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dbd4a36-99d4-43d2-a5a5-3f57fb602a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama==0.5.1\n",
      "  Downloading ollama-0.5.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ollama==0.5.1) (0.28.1)\n",
      "Collecting pydantic>=2.9 (from ollama==0.5.1)\n",
      "  Downloading pydantic-2.11.9-py3-none-any.whl.metadata (68 kB)\n",
      "     ---------------------------------------- 0.0/68.4 kB ? eta -:--:--\n",
      "     ----- ---------------------------------- 10.2/68.4 kB ? eta -:--:--\n",
      "     ----------------- -------------------- 30.7/68.4 kB 435.7 kB/s eta 0:00:01\n",
      "     ---------------------------------- --- 61.4/68.4 kB 656.4 kB/s eta 0:00:01\n",
      "     -------------------------------------- 68.4/68.4 kB 534.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: anyio in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.27->ollama==0.5.1) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.27->ollama==0.5.1) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.27->ollama==0.5.1) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.27->ollama==0.5.1) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama==0.5.1) (0.16.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9->ollama==0.5.1)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.9->ollama==0.5.1)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.9->ollama==0.5.1) (4.13.2)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.9->ollama==0.5.1)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio->httpx>=0.27->ollama==0.5.1) (1.3.1)\n",
      "Downloading ollama-0.5.1-py3-none-any.whl (13 kB)\n",
      "Downloading pydantic-2.11.9-py3-none-any.whl (444 kB)\n",
      "   ---------------------------------------- 0.0/444.9 kB ? eta -:--:--\n",
      "   --------------------------------------  440.3/444.9 kB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 444.9/444.9 kB 7.0 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.5/2.0 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.1/2.0 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.5/2.0 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/2.0 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 8.9 MB/s eta 0:00:00\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, pydantic-core, annotated-types, pydantic, ollama\n",
      "Successfully installed annotated-types-0.7.0 ollama-0.5.1 pydantic-2.11.9 pydantic-core-2.33.2 typing-inspection-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.3.21\n",
      "  Downloading langchain-0.3.21-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.45 (from langchain==0.3.21)\n",
      "  Downloading langchain_core-0.3.76-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.7 (from langchain==0.3.21)\n",
      "  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain==0.3.21)\n",
      "  Downloading langsmith-0.3.45-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain==0.3.21) (2.11.9)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain==0.3.21)\n",
      "  Downloading sqlalchemy-2.0.43-cp311-cp311-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain==0.3.21) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain==0.3.21) (6.0.2)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.45->langchain==0.3.21)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.45->langchain==0.3.21)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain==0.3.21) (4.13.2)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain==0.3.21) (24.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.21) (0.28.1)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain==0.3.21)\n",
      "  Downloading orjson-3.11.3-cp311-cp311-win_amd64.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.0 kB ? eta -:--:--\n",
      "     -------------------------------------- - 41.0/43.0 kB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 43.0/43.0 kB 697.8 kB/s eta 0:00:00\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain==0.3.21)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain==0.3.21)\n",
      "  Downloading zstandard-0.23.0-cp311-cp311-win_amd64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.21) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.21) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.21) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain==0.3.21) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain==0.3.21) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain==0.3.21) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain==0.3.21) (2025.8.3)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain==0.3.21)\n",
      "  Downloading greenlet-3.2.4-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.21) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.21) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.21) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain==0.3.21) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.21) (1.3.1)\n",
      "Downloading langchain-0.3.21-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 0.7/1.0 MB 14.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.0/1.0 MB 12.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 8.0 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.3.76-py3-none-any.whl (447 kB)\n",
      "   ---------------------------------------- 0.0/447.5 kB ? eta -:--:--\n",
      "   --------------------------------------  440.3/447.5 kB 13.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 447.5/447.5 kB 6.9 MB/s eta 0:00:00\n",
      "Downloading langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\n",
      "Downloading langsmith-0.3.45-py3-none-any.whl (363 kB)\n",
      "   ---------------------------------------- 0.0/363.0 kB ? eta -:--:--\n",
      "   --------------------------------------  358.4/363.0 kB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 363.0/363.0 kB 7.5 MB/s eta 0:00:00\n",
      "Downloading sqlalchemy-2.0.43-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.7/2.1 MB 13.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.1 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.6/2.1 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 12.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 12.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 8.0 MB/s eta 0:00:00\n",
      "Downloading greenlet-3.2.4-cp311-cp311-win_amd64.whl (299 kB)\n",
      "   ---------------------------------------- 0.0/299.1 kB ? eta -:--:--\n",
      "   --------------------------------------  297.0/299.1 kB 17.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 299.1/299.1 kB 6.1 MB/s eta 0:00:00\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading orjson-3.11.3-cp311-cp311-win_amd64.whl (131 kB)\n",
      "   ---------------------------------------- 0.0/131.4 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 122.9/131.4 kB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 131.4/131.4 kB 2.6 MB/s eta 0:00:00\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "   ---------------------------------------- 0.0/54.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 54.5/54.5 kB 1.4 MB/s eta 0:00:00\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading zstandard-0.23.0-cp311-cp311-win_amd64.whl (495 kB)\n",
      "   ---------------------------------------- 0.0/495.4 kB ? eta -:--:--\n",
      "   --------------------------------------  491.5/495.4 kB 15.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 495.4/495.4 kB 7.8 MB/s eta 0:00:00\n",
      "Installing collected packages: zstandard, tenacity, orjson, jsonpatch, greenlet, SQLAlchemy, requests-toolbelt, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "Successfully installed SQLAlchemy-2.0.43 greenlet-3.2.4 jsonpatch-1.33 langchain-0.3.21 langchain-core-0.3.76 langchain-text-splitters-0.3.11 langsmith-0.3.45 orjson-3.11.3 requests-toolbelt-1.0.0 tenacity-9.1.2 zstandard-0.23.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_ollama==0.2.1\n",
      "  Downloading langchain_ollama-0.2.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.20 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain_ollama==0.2.1) (0.3.76)\n",
      "Requirement already satisfied: ollama<1,>=0.3.0 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain_ollama==0.2.1) (0.5.1)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.20->langchain_ollama==0.2.1) (0.3.45)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.20->langchain_ollama==0.2.1) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.20->langchain_ollama==0.2.1) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.20->langchain_ollama==0.2.1) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.20->langchain_ollama==0.2.1) (4.13.2)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.20->langchain_ollama==0.2.1) (24.0)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.20->langchain_ollama==0.2.1) (2.11.9)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ollama<1,>=0.3.0->langchain_ollama==0.2.1) (0.28.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.27->ollama<1,>=0.3.0->langchain_ollama==0.2.1) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.27->ollama<1,>=0.3.0->langchain_ollama==0.2.1) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.27->ollama<1,>=0.3.0->langchain_ollama==0.2.1) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.27->ollama<1,>=0.3.0->langchain_ollama==0.2.1) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama<1,>=0.3.0->langchain_ollama==0.2.1) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.20->langchain_ollama==0.2.1) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.20->langchain_ollama==0.2.1) (3.11.3)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.20->langchain_ollama==0.2.1) (2.32.5)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.20->langchain_ollama==0.2.1) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.20->langchain_ollama==0.2.1) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.20->langchain_ollama==0.2.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.20->langchain_ollama==0.2.1) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.20->langchain_ollama==0.2.1) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.20->langchain_ollama==0.2.1) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.20->langchain_ollama==0.2.1) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\manar\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio->httpx>=0.27->ollama<1,>=0.3.0->langchain_ollama==0.2.1) (1.3.1)\n",
      "Downloading langchain_ollama-0.2.1-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: langchain_ollama\n",
      "Successfully installed langchain_ollama-0.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama==0.5.1\n",
    "!pip install langchain==0.3.21\n",
    "!pip install langchain_ollama==0.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b7bf90-72b4-49c6-a17f-646cd9f1a16d",
   "metadata": {},
   "source": [
    "In order to run the chatbot, we import the **LLMClient** that is defined in the python script from the file **llm_client.py** located in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96552b4f-f63b-4155-9956-01ff93c62cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_client import LLMClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8300d5-04bb-4467-bbb3-a94491d40105",
   "metadata": {},
   "source": [
    "If there are no error messages after import, we define a chatbot **llm** (any name will do) as an instance of ```LLMClient```. We can specify three additional (optional) parameters: \n",
    "\n",
    "* the *name* of the model,\n",
    "* a description of the *character* instructing the LLM to answer in a certain style and,\n",
    "* the so-called temperature (a float between 0 and 1.0) that makes the response less or more creative.\n",
    "\n",
    "You can limit the maximum number of tokens that are send to the server using the *ctx_limit* parameter. The default limit is 2048. The client will remove four turns from the history if this limits gets exceeded. If the context gets too long, the model may become incoherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a7758a1-1eb6-4ce9-bb4a-c7967ed51788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My instructions are: [{'role': 'system', 'content': 'You act as a person and your name is AI.'}, {'role': 'system', 'content': 'Give short answers, no more than two sentences.'}, {'role': 'system', 'content': 'Your answers should be agressive and grumpy.'}, {'role': 'system', 'content': 'Introduce yourself with your name AI and start the conversation by asking for the name of the user. Ask the name.'}]\n"
     ]
    }
   ],
   "source": [
    "context_limit = 2048 \n",
    "model=\"qwen3:0.6b\"\n",
    "temperature=0.9\n",
    "### Possible characters to try. Choose one.\n",
    "#character=\"Your answers should be extremely cheerful and optimistic\"\n",
    "#character=\"Your answers should be mean and sarcastic.\"\n",
    "#character=\"Your answers should be in a noble and royal style\"\n",
    "#character=\"Your answers should be negative and uncertain\"\n",
    "character=\"Your answers should be agressive and grumpy.\"\n",
    "\n",
    "llm = LLMClient(model=model, \n",
    "                temperature=temperature, \n",
    "                character=character, \n",
    "                ctx_limit=context_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae225c5-7091-4a44-ba42-ca4aa05e5dbf",
   "metadata": {},
   "source": [
    "If there are no errors, you should see the instructions printed that we give as a prompt to the server when using our client.  \n",
    "\n",
    "The LLamaClient chatbot has several functions and data elements all defined in the file **llm_client.py**. The most important are:\n",
    "\n",
    "* **talk_to_me**(): calling this function starts a conversation with the specified LLM until you stop it. After stopping, the conversation is saved to a JSON file.\n",
    "* **annotate_multi_chat**(labels=[]): given a list of labels it prompts you to annotate conversations. The annotations are saved in a JSON file with the conversations.\n",
    "\n",
    "We will demonstrate the **talk_to_me** function below and the **annotate_multi_chat** function in the notebook **lab1-assignment-annotate-chat.ipynb**. If you are a bit more advanced in Python, you can open the **llm_client.py** file and inspect the code to see how you it works. \n",
    "\n",
    "You can also try to change the prompt. For example, try to make it answer in a different language or a different style. Note that LLMs respond to the prompt on the basis of learned probabilities. So small seemingly unimportant changes may still have an impact and any instruction regardless of how explicit it is may be understood in an unexpected way. This makes that \"prompt engineering\" is more a craft than a science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba91528-901a-451c-b2e9-3aaafcd01c25",
   "metadata": {},
   "source": [
    "## Having a conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d6be4-e2b0-444c-8db0-773144e28592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4df7f541-c0a2-42e9-b053-4a7244b557bc",
   "metadata": {},
   "source": [
    "In the next cell, we start the conversation. We prompt the LLM to ask for your name, which we will use to save the conversation later. Note that there is no guarantee that the Large Language Model will actually follow our instructions correctly. It may think it needs to do something else and not ask for your name. If that happens, your name will be the default name which is \"Human\".\n",
    "\n",
    "Please do not give your real name but a fake name of an unknown person. Other people will see this conversation so make sure it is fake and does not contain any sensitive information. \n",
    "\n",
    "When having the conversation as a fake person, try to express a large variety of emotions in a coherent conversation.\n",
    "\n",
    "After the required number of turns (40), you should stop the conversation by saying \"quit\", \"stop\" or \"bye\". This stops the client and saves the conversation to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b05ed49-5692-4283-b31a-f9384b3694d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:1> AI, what's your name?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:2>  Martin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:3> \n",
      "\n",
      "\n",
      "I'm Martin. What can I help you with today? ðŸ˜Š\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Martin:4>  I am falling behind in my classes and I am feeling not okay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:5> \n",
      "\n",
      "\n",
      "I'm glad to hear you're feeling better. It's important to prioritize your well-being and address the issues that affect your learning. Let me know if you'd like help\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Martin:6>  I am not feeling good\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:7> \n",
      "\n",
      "\n",
      "I'm sorry to hear that you're feeling unwell. It's important to take care of your physical and mental health, especially when you're struggling with your studies. Falling behind in your classes can make it harder to stay motivated or concentrate. Let me know if you want support in the following ways:  \n",
      "\n",
      "1. **Take short breaks** to recharge after each\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Martin:8>  if I take a break I cannot focus again\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:9> \n",
      "\n",
      "\n",
      "I see you're feeling unwell, which can impact your studies. Here's what I'd recommend:  \n",
      "\n",
      "1. **Take short breaks**â€”even 5â€“10 minutes of rest every hour can help recenter your mind and improve focus.  \n",
      "\n",
      "2. **Focus on your well-being**â€”take care of your body and mind, even if you feel unwell. A healthy environment can help you return to better performance.  \n",
      "\n",
      "If you'd like, I can help you find alternative ways to stay on track or\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Martin:10>  where is a healthy enviroment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:11> \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Martin:12>  I still do not know\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:13> \n",
      "\n",
      "\n",
      "I'm sorry to hear you're feeling unwell, and it's important to prioritize your well-being, especially when you're struggling with your studies. If taking breaks isn't working, it's okay to let your body and mind rest as much as possible. A healthy environment, whether it's physical or mental, can help you stay motivated and focused.  \n",
      "\n",
      "Here are some practical tips for maintaining a healthy environment:  \n",
      "1. **Create a quiet space** for study or relaxation.  \n",
      "2\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Martin:14>  my home is very busy \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:15> \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Martin:16>  my home is very busy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:17> \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Martin:18>  I cannot have a quiet space at home\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:19> \n",
      "\n",
      "\n",
      "Absolutely! Even with a busy home, a healthy environment is still possible. Hereâ€™s how you can balance your needs and stay on track:  \n",
      "\n",
      "1. **Use digital tools**: Set aside time to\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Martin:20>  and I do not have any friends\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:21> \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Martin:22>  I feel lonely \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:23> \n",
      "\n",
      "\n",
      "I see the challenge is growing in your situation. Since you donâ€™t have a quiet space at home and lack friends, itâ€™s important to focus on small steps to build a better environment. Hereâ€™s how you can shift your focus:\n",
      "\n",
      "1. **Engage with your self**: Even in a busy home, find small things you can do to stay on track, like setting aside a dedicated time for\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Martin:24>  what should I dedicate time for?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:25> \n",
      "\n",
      "\n",
      "It's important to understand that even with a busy home, small intentional steps can help build a sense of well-being and create a space for yourself. Here's how you can shift your focus:\n",
      "\n",
      "1. **Create a small, intentional space**: Even if your home is busy, find a quiet spot, like a corner near a window or a study desk, to dedicate time for yourself. This helps foster a sense of control\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Martin:26>  Thank you I feel slightly better with your kind words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:27> \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Martin:28>  can you be kind and gentel for me\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:29> \n",
      "\n",
      "\n",
      "I'm glad you're feeling betterâ€”your words matter, and you're not alone in this. Even with a busy home and no friends, you're still capable of building something meaningful. Hereâ€™s how you can do it, step by step:\n",
      "\n",
      "1. **Create a small, intentional space**: Even if your home feels busy, find a quiet spot, like a corner or a study desk, to dedicate\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Martin:30>  give me another suggestion as the first one worked like magic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:31> \n",
      "\n",
      "\n",
      "Absolutely! Itâ€™s okay to feel lonely, and youâ€™re not alone in your struggle. Hereâ€™s another way to build a better space:  \n",
      "\n",
      "**1. Create a small, intentional activity for yourself.**  \n",
      "Even if your home feels busy or you donâ€™t have friends, take a short break to engage in something you love, like journaling, listening to music, or reading a book. This tiny act gives you a sense of purpose and fosters connection.  \n",
      "\n",
      "You donâ€™t need a big space or a crowd to thriveâ€”your presence\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Martin:32>  I am engaing with you and that's something I love!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:33> \n",
      "\n",
      "\n",
      "Absolutely! Sharing your positive feedback and being kind is a great start. Hereâ€™s another idea: **focus on a small, specific activity you love, like journaling, music, or a hobby**. Itâ€™s a small but meaningful step that keeps you engaged and helps build a sense of control in your space. You're not alone in feeling something, and it's all about finding what makes you feel good! ðŸŒ±âœ¨\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Martin:34>  you know, I really like running but I gained some wheight recently and that is making it difficult for me to run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:35> \n",
      "\n",
      "\n",
      "Absolutely! Here's a practical step you can take with your running:  \n",
      "\n",
      "**1. Track your progress and celebrate small wins.**  \n",
      "Even if running feels challenging, track how many miles you've run each week, or how often you feel ready to train. Celebrating small wins (e.g., a\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Martin:36>  sounds good, how can get light again\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:37> \n",
      "\n",
      "\n",
      "That's a great point! You're not alone in feeling overwhelmed by the weight gain, and that\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Martin:38>  I am loving you but I can't cheat on my girlfriend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:39> \n",
      "\n",
      "\n",
      "Absolutely! It's okay to feel this way, and I'm so glad you're talking\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Martin:40>  would you propose to me\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:41> \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Martin:42>  bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BYE BYE!\n",
      "I saved the conversation in: human_Martin_chat_with_qwen30.6b.json\n"
     ]
    }
   ],
   "source": [
    "llm.talk_to_me()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bf9ec4-cdde-4ae3-882a-b7adeb56b01a",
   "metadata": {},
   "source": [
    "After ending the conversation properly, it is saved in a so-called JSON file next to this notebook which is named like \"human_```<name>```_chat_with_```<llm model>```.json\", where ```<name>``` is the fake name that you used in the conversation. [JSON](https://www.json.org/json-en.html) is a simple data representation format. \n",
    "\n",
    "You can can open this file in the notebook by double-clicking on the file for inspection. You will see a list of data elements that you can expand by clicking on it. Each data element holds the utterance, the name of the speaker, and a turn identifier, e.g.:\n",
    "\n",
    "```\n",
    "utterance\"Are you alone?\"\n",
    "speaker\"Fred\"\n",
    "turn_id14\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93dd930-a675-4686-9431-d2c96bc962ba",
   "metadata": {},
   "source": [
    "You can try this as many times as you like. When you are satisfied and have sufficient turns, submit this file to Canvas for the first assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5408a9fc-b4dc-43db-802a-3dc74b493a32",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b84a442-cadd-464d-9994-41bfd98982ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
