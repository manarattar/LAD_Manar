{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73cab140",
   "metadata": {},
   "source": [
    "# Part III – LLM Dialogue-Act Annotation with Qwen (Ollama)\n",
    "\n",
    "This notebook:\n",
    "- Loads my mini-corpus of utterances.\n",
    "- Uses a local LLM (Qwen via Ollama) to annotate dialogue acts.\n",
    "- Performs iterative prompt refinement (multiple prompt versions).\n",
    "- Annotates the full dataset with the best prompt.\n",
    "- Computes inter-annotator agreement (Cohen's κ) between humans and the LLM.\n",
    "\n",
    "**Note:** This template assumes you have Ollama installed and a `qwen` model available.\n",
    "You may need to adjust file paths and model names to match your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b352bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ollama\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6909b7b",
   "metadata": {},
   "source": [
    "## 1. Load data\n",
    "\n",
    "This cell loads the utterances exported from ELAN.\n",
    "\n",
    "It assumes the file is a tab-delimited text file with the following columns:\n",
    "\n",
    "1. tier speaker name\n",
    "2. participant name\n",
    "3. start time (string)\n",
    "4. start time (seconds)\n",
    "5. end time (string)\n",
    "6. end time (seconds)\n",
    "7. duration (string)\n",
    "8. duration (seconds)\n",
    "9. utterance text\n",
    "\n",
    "If you later add human annotation labels (e.g. `human1_label`, `human2_label`), you can\n",
    "merge them into this DataFrame or load a different file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fff17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "utter_path = 'AttarA2_with_human_annotation.txt'\n",
    "\n",
    "df = pd.read_csv(\n",
    "    utter_path,\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=['tier_speaker', 'speaker', 'start_str', 'start_sec', 'end_str', 'end_sec', 'dur_str', 'dur_sec', 'utterance'],\n",
    "    engine='python'\n",
    ")\n",
    "\n",
    "print(df.head())\n",
    "print('Total utterances:', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c163f7",
   "metadata": {},
   "source": [
    "### Optional: Add human label columns\n",
    "\n",
    "If you already have human annotations in a separate file, you can merge them here.\n",
    "For now, we create placeholder columns `human1_label` and `human2_label` that you can\n",
    "fill externally (e.g. in Excel) and re-load later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee9fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'human1_label' not in df.columns:\n",
    "    df['human1_label'] = pd.NA\n",
    "if 'human2_label' not in df.columns:\n",
    "    df['human2_label'] = pd.NA\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bafdcda",
   "metadata": {},
   "source": [
    "## 2. Define dialogue-act labels\n",
    "\n",
    "These are the labels we will use for annotation, based on your scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd18aef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DA_LABELS = [\n",
    "    'STATEMENT',\n",
    "    'QUESTION',\n",
    "    'ANSWER',\n",
    "    'ACKNOWLEDGEMENT/BACKCHANNEL',\n",
    "    'DIRECTIVE/REQUEST',\n",
    "    'REPAIR/CLARIFICATION',\n",
    "    'EXPRESSIVE/EMOTIVE',\n",
    "    'APOLOGY',\n",
    "    'GREETING',\n",
    "    'GOODBYE/CLOSING',\n",
    "    'OTHER'\n",
    "]\n",
    "DA_LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a413803",
   "metadata": {},
   "source": [
    "## 3. Helper: call Qwen via Ollama\n",
    "\n",
    "This function sends a batch of utterances to the model with a given prompt and expects\n",
    "JSON output of the form:\n",
    "\n",
    "```json\n",
    "[{\"index\": 1, \"label\": \"STATEMENT\"}, ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef7b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_batch_with_llm(utterances, prompt, temperature=0.3, model_name='qwen'):\n",
    "    joined = '\\n'.join([f\"{i+1}. {u}\" for i, u in enumerate(utterances)])\n",
    "\n",
    "    full_prompt = f\"\"\"{prompt}\n",
    "\n",
    "Here are the utterances to annotate:\n",
    "{joined}\n",
    "\n",
    "Return ONLY valid JSON as a list of objects:\n",
    "[\n",
    "  {{\"index\": 1, \"label\": \"STATEMENT\"}},\n",
    "  ...\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=[{'role': 'user', 'content': full_prompt}],\n",
    "        options={'temperature': temperature}\n",
    "    )\n",
    "\n",
    "    text = response['message']['content'].strip()\n",
    "\n",
    "    try:\n",
    "        annotations = json.loads(text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print('Failed to parse JSON from model output:')\n",
    "        print(text)\n",
    "        raise e\n",
    "\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990dc6e8",
   "metadata": {},
   "source": [
    "## 4. Prompt Version 1 – initial instructions\n",
    "\n",
    "This is the base prompt that defines your labels and how the model should respond.\n",
    "You will refine it over several versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c466fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_v1 = '''\n",
    "You are annotating dialogue acts in a conversation.\n",
    "\n",
    "Use EXACTLY ONE label per utterance, chosen from:\n",
    "STATEMENT, QUESTION, ANSWER, ACKNOWLEDGEMENT/BACKCHANNEL,\n",
    "DIRECTIVE/REQUEST, REPAIR/CLARIFICATION,\n",
    "EXPRESSIVE/EMOTIVE, APOLOGY, GREETING,\n",
    "GOODBYE/CLOSING, OTHER.\n",
    "\n",
    "Definitions:\n",
    "- STATEMENT: provides information, description, or opinion.\n",
    "- QUESTION: requests information or clarification.\n",
    "- ANSWER: directly responds to a question or confirmation request.\n",
    "- ACKNOWLEDGEMENT/BACKCHANNEL: signals attention, understanding, or agreement\n",
    "  without adding new content (e.g. \"yeah\", \"mm-hmm\", \"right\").\n",
    "- DIRECTIVE/REQUEST: asks the other speaker to do something or to clarify something.\n",
    "- REPAIR/CLARIFICATION: corrects or reformulates previous speech, or asks for clarification\n",
    "  (e.g. \"No, I meant last week\", \"Wait, who was there?\").\n",
    "- EXPRESSIVE/EMOTIVE: conveys emotional stance, evaluation, or affective reaction\n",
    "  (e.g. \"That was amazing!\", \"Ugh, I hated that part\").\n",
    "- APOLOGY: acknowledges fault or expresses regret.\n",
    "- GREETING: opens the interaction or marks social connection.\n",
    "- GOODBYE/CLOSING: signals the end of the interaction or closing the topic.\n",
    "- OTHER: does not fit the categories above (e.g. laughter, filler noises, unintelligible speech).\n",
    "\n",
    "Rules:\n",
    "- Assign EXACTLY ONE label per utterance.\n",
    "- Only use labels from the set given above.\n",
    "- If unsure, choose the best fitting category and do NOT invent new labels.\n",
    "'''\n",
    "print(prompt_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d78eb9",
   "metadata": {},
   "source": [
    "## 5. Test Prompt V1 on a small subset\n",
    "\n",
    "We start with 10 utterances to see how well the model follows the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e23d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.iloc[:10].copy()\n",
    "test_utterances = test_df['utterance'].tolist()\n",
    "\n",
    "annotations_v1 = annotate_batch_with_llm(\n",
    "    utterances=test_utterances,\n",
    "    prompt=prompt_v1,\n",
    "    temperature=0.3,\n",
    "    model_name='qwen'\n",
    ")\n",
    "\n",
    "annotations_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab0882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach V1 annotations to the test DataFrame for inspection\n",
    "idx_to_label_v1 = {ann['index']: ann['label'] for ann in annotations_v1}\n",
    "\n",
    "test_df['llm_v1_label'] = [idx_to_label_v1[i+1] for i in range(len(test_df))]\n",
    "test_df[['utterance', 'human1_label', 'human2_label', 'llm_v1_label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a04553a",
   "metadata": {},
   "source": [
    "## 6. Prompt Refinement Log (fill in as you iterate)\n",
    "\n",
    "- **Version 1**: Basic definitions of all dialogue-act labels.\n",
    "  - Problems observed: (e.g., short \"yeah\" labeled as STATEMENT).\n",
    "- **Version 2**: ...\n",
    "- **Version 3**: ...\n",
    "- **Version 4**: ...\n",
    "- **Version 5**: ...\n",
    "\n",
    "You should create new prompt variables `prompt_v2`, `prompt_v3`, etc., and\n",
    "re-run the same `test_utterances` to compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92236e1",
   "metadata": {},
   "source": [
    "### Example: Prompt Version 2\n",
    "\n",
    "Below is an example refinement that you can further adjust based on the errors you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0688f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_v2 = prompt_v1 + '''\n",
    "\n",
    "Additional rules:\n",
    "- If an utterance consists only of very short tokens like \"yeah\", \"mm-hmm\", \"uh-huh\", \"right\",\n",
    "  and does not add new information, ALWAYS label it as ACKNOWLEDGEMENT/BACKCHANNEL.\n",
    "- If an utterance clearly asks for information or ends with a question mark,\n",
    "  label it as QUESTION, not STATEMENT.\n",
    "'''\n",
    "print(prompt_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2046a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_v2 = annotate_batch_with_llm(\n",
    "    utterances=test_utterances,\n",
    "    prompt=prompt_v2,\n",
    "    temperature=0.2,\n",
    "    model_name='qwen'\n",
    ")\n",
    "\n",
    "idx_to_label_v2 = {ann['index']: ann['label'] for ann in annotations_v2}\n",
    "test_df['llm_v2_label'] = [idx_to_label_v2[i+1] for i in range(len(test_df))]\n",
    "\n",
    "test_df[['utterance', 'human1_label', 'human2_label', 'llm_v1_label', 'llm_v2_label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ee3aeb",
   "metadata": {},
   "source": [
    "## 7. Choose a final prompt\n",
    "\n",
    "After trying at least 5 prompt versions, assign your best-performing one to `final_prompt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdf7767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: after refining up to prompt_v5 (for example), set your final prompt here\n",
    "final_prompt = prompt_v2  # replace with your best prompt, e.g., prompt_v5\n",
    "print(final_prompt[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f84350",
   "metadata": {},
   "source": [
    "## 8. Annotate the full dataset with the final prompt\n",
    "\n",
    "This function runs the LLM over the entire dataset in batches and stores\n",
    "the predicted label in a new column `llm_label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e532c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_full_df_with_llm(df, prompt, batch_size=20, temperature=0.2, model_name='qwen'):\n",
    "    llm_labels = []\n",
    "    n = len(df)\n",
    "\n",
    "    for start in range(0, n, batch_size):\n",
    "        end = min(start + batch_size, n)\n",
    "        batch_utts = df['utterance'].iloc[start:end].tolist()\n",
    "        annotations = annotate_batch_with_llm(\n",
    "            utterances=batch_utts,\n",
    "            prompt=prompt,\n",
    "            temperature=temperature,\n",
    "            model_name=model_name\n",
    "        )\n",
    "        idx_to_label = {ann['index']: ann['label'] for ann in annotations}\n",
    "        batch_labels = [idx_to_label[i+1] for i in range(len(batch_utts))]\n",
    "        llm_labels.extend(batch_labels)\n",
    "        print(f'Annotated {end}/{n} utterances')\n",
    "\n",
    "    return llm_labels\n",
    "\n",
    "df['llm_label'] = annotate_full_df_with_llm(\n",
    "    df=df,\n",
    "    prompt=final_prompt,\n",
    "    batch_size=20,\n",
    "    temperature=0.2,\n",
    "    model_name='qwen'\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867701a6",
   "metadata": {},
   "source": [
    "## 9. Save LLM-annotated data\n",
    "\n",
    "Save the DataFrame with the new `llm_label` column for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13299a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = 'AttarA2_with_llm_labels.csv'\n",
    "df.to_csv(out_path, index=False)\n",
    "print('Saved:', out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d85689",
   "metadata": {},
   "source": [
    "## 10. Compute Cohen's κ\n",
    "\n",
    "This section computes agreement between:\n",
    "- Human 1 vs Human 2\n",
    "- Human 1 vs LLM\n",
    "- Human 2 vs LLM\n",
    "\n",
    "Make sure you have filled in `human1_label` and `human2_label` before running this.\n",
    "If they are still empty, κ will not be meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f624aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df['human1_label'].notna() & df['human2_label'].notna() & df['llm_label'].notna()\n",
    "eval_df = df[mask].copy()\n",
    "\n",
    "if len(eval_df) == 0:\n",
    "    print('No rows with all three labels present yet. Fill human1_label and human2_label first.')\n",
    "else:\n",
    "    h1 = eval_df['human1_label']\n",
    "    h2 = eval_df['human2_label']\n",
    "    llm_labs = eval_df['llm_label']\n",
    "\n",
    "    kappa_h1_h2 = cohen_kappa_score(h1, h2)\n",
    "    kappa_h1_llm = cohen_kappa_score(h1, llm_labs)\n",
    "    kappa_h2_llm = cohen_kappa_score(h2, llm_labs)\n",
    "\n",
    "    print(\"Cohen's κ (Human1 vs Human2):\", kappa_h1_h2)\n",
    "    print(\"Cohen's κ (Human1 vs LLM):   \", kappa_h1_llm)\n",
    "    print(\"Cohen's κ (Human2 vs LLM):   \", kappa_h2_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0ca6af",
   "metadata": {},
   "source": [
    "## 11. Confusion matrices\n",
    "\n",
    "If you have enough labeled data, you can visualise where the LLM disagrees\n",
    "with human annotators using confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55488eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels, title):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(\n",
    "        xticks=np.arange(cm.shape[1]),\n",
    "        yticks=np.arange(cm.shape[0]),\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "        ylabel='True label',\n",
    "        xlabel='Predicted label',\n",
    "        title=title\n",
    "    )\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, cm[i, j],\n",
    "                    ha='center', va='center',\n",
    "                    color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if len(eval_df) > 0:\n",
    "    plot_confusion_matrix(h1, h2, DA_LABELS, 'Human1 vs Human2')\n",
    "    plot_confusion_matrix(h1, llm_labs, DA_LABELS, 'Human1 vs LLM')\n",
    "    plot_confusion_matrix(h2, llm_labs, DA_LABELS, 'Human2 vs LLM')\n",
    "else:\n",
    "    print('Not enough labeled data to plot confusion matrices yet.')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
